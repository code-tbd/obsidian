# 1 I/O 模型简介
I/O模型主要为以下五种：

- 阻塞 I/O (Blocking I/O) 
- 非阻塞 I/O (Nonblocking I/O) 
- I/O 多路复用 (I/O multiplexing) 
- 信号驱动 I/O (Signal driven I/O) 
- 异步 I/O (Asynchronous I/O)  

其中前四种 I/O 模型均为同步 I/O，判断 I/O 是否同步的根本依据是**内核空间**和**用户空间**之间的拷贝是否会阻塞**应用进程**。
![5种 I/O 模型的比较](https://cdn.nlark.com/yuque/0/2022/png/32538979/1661151264747-d4a196f9-94f2-4a2f-9174-66d77fbcd887.png#averageHue=%23f3f3f3&clientId=uba32399e-a230-4&from=paste&height=427&id=u7365f0c4&originHeight=534&originWidth=1058&originalType=binary&ratio=1&rotation=0&showTitle=true&size=142611&status=done&style=shadow&taskId=ue5617c46-9007-487f-b63a-984a9204ba5&title=5%E7%A7%8D%20I%2FO%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AF%94%E8%BE%83&width=846.4 "5种 I/O 模型的比较")
## 1.1 BIO
应用进程向内核发起系统调用，程序会立即**阻塞**，等待数据到达内核并将数据拷贝至用户内存之后，应用进程恢复，开始处理数据。
在整个过程中，应用进程只能对一个 I/O 连接进行操作。
![阻塞式 I/O 模型](https://cdn.nlark.com/yuque/0/2022/png/32538979/1661151357235-7e3bc071-bbe0-420c-b386-87226d69c826.png#averageHue=%23f6f6f6&clientId=uba32399e-a230-4&from=paste&height=320&id=u8a2e8446&originHeight=400&originWidth=828&originalType=binary&ratio=1&rotation=0&showTitle=true&size=78182&status=done&style=shadow&taskId=ue87b20e4-a690-48d7-828d-d07a7b9962d&title=%E9%98%BB%E5%A1%9E%E5%BC%8F%20I%2FO%20%E6%A8%A1%E5%9E%8B&width=662.4 "阻塞式 I/O 模型")
## 1.2 NIO
应用进程向内核发起系统调用，内核会立即返回数据是否准备完毕，若未准备完毕，应用进程会不断发起调用，这一过程称为**轮询（poll）**，此过程应用进程会占用 cpu 资源。当内核中数据准备完成时，应用进程开始**阻塞**，直到数据从内核拷贝至用户内存。
同 BIO 一样，NIO 一次只能操作一个 I/O 连接。
![非阻塞式 I/O 模型](https://cdn.nlark.com/yuque/0/2022/png/32538979/1661151582899-12451551-05b1-4c25-b353-246ebb48347d.png#averageHue=%23f2f2f2&clientId=uba32399e-a230-4&from=paste&height=371&id=u284e45fa&originHeight=464&originWidth=926&originalType=binary&ratio=1&rotation=0&showTitle=true&size=140869&status=done&style=shadow&taskId=u1aec91f5-bdc9-4ec6-b6f7-a29ba84d850&title=%E9%9D%9E%E9%98%BB%E5%A1%9E%E5%BC%8F%20I%2FO%20%E6%A8%A1%E5%9E%8B&width=740.8 "非阻塞式 I/O 模型")
## 1.3 I/O 多路复用
BIO 和 NIO 都是直接阻塞在系统调用上，而 I/O 多路复用不同，他是阻塞在特定的实现函数上（如 select、poll 等）。
应用程序通过特定的实现函数，向内核发起调用并立即**阻塞**等待具体实现函数返回，实现函数可以同时等待多个 I/O 连接，当等待的 I/O 连接中至少有一个数据准备完毕时，函数返回。应用程序再次请求向内核请求读取准备就绪的 I/O 连接，并再次**阻塞**，直到一个或多个数据从内核拷贝至用户内存。
![I/O 复用模型](https://cdn.nlark.com/yuque/0/2022/png/32538979/1661151922168-ea9359fe-3e27-46d7-9927-ecb4809a3b41.png#averageHue=%23f1f1f1&clientId=uba32399e-a230-4&from=paste&height=359&id=u758fd916&originHeight=449&originWidth=920&originalType=binary&ratio=1&rotation=0&showTitle=true&size=137319&status=done&style=shadow&taskId=u15e74b27-3e47-4783-9308-cc3b5ec9cff&title=I%2FO%20%E5%A4%8D%E7%94%A8%E6%A8%A1%E5%9E%8B&width=736 "I/O 复用模型")
# 2 I/O 多路复用模型
linux 上常见的多路复用模型有 select、poll 和 epoll；unix 上常见的多路复用模型有 kqueue。
## 2.1 select
select 是 linux 系统中最早采用的 I/O 多路复用模型，linux 版本为 [v5.19](https://github.com/torvalds/linux/tree/v5.19)。
### 2.1.1 数据结构/函数原型
```c
#define FD_SETSIZE     256
typedef struct {
	uint32_t fd32[(FD_SETSIZE + 31) / 32];
} fd_set;
```
FD_SETSIZE 为最大 set 大小，默认为 256（网上文章大多为 1024，可能是 linux 版本不同造成的差异）。fd_set是一组 fd 的 bitmap，其下标与 fd 编号相对应，例`fd_set->fd32[0]`表示 fd 编号为 0 的值，其值的具体意义在 select 不同阶段不相同。

---

```c
// 将 set 中下标 fd 处置空
void FD_CLR(int fd, fd_set *set);
#define FD_CLR(fd, set) do {                                    \
		fd_set *__set = (set);                                  \
		int __fd = (fd);                                        \
		if (__fd >= 0)                                          \
			__set->fd32[__fd / 32] &= ~(1U << (__fd & 31)); 	\
    } while (0)
```
```c
// 将set 中下标 fd 处置位
void FD_SET(int fd, fd_set *set);
#define FD_SET(fd, set) do {                                    \
		fd_set *__set = (set);                                  \
		int __fd = (fd);                                        \
		if (__fd >= 0)                                          \
			__set->fd32[__fd / 32] |= 1U << (__fd & 31);        \
	} while (0)
```
```c
// 判断给定的 fd 编号是否准备就绪
int  FD_ISSET(int fd, fd_set *set);
#define FD_ISSET(fd, set) ({                                          \
		fd_set *__set = (set);                                        \
		int __fd = (fd);                                              \
		int __r = 0;                                                  \
		if (__fd >= 0)                                                \
			__r = !!(__set->fd32[__fd / 32] & 1U << (__fd & 31));     \
		__r;                                                          \
	})
```
```c
// 将整个 set 置空
void FD_ZERO(fd_set *set);
#define FD_ZERO(set) do {                                       \
		fd_set *__set = (set);                                  \
		int __idx;                                              \
		for (__idx = 0; __idx < (FD_SETSIZE+31) / 32; __idx ++) \
			__set->fd32[__idx] = 0;                             \
	} while (0)
```
```c
// nfds 传入的rfds/wfds/efds集合中fd编号最大值加1
// rfds/wfds/efds 读/写/异常fd集合
// timeval 超时时间
int select(int nfds, fd_set *rfds, fd_set *wfds, fd_set *efds, struct timeval *timeout);
```
用户程序准备好一系列 fd 编号，根据需要的读写等操作，**将对应集合下标为 fd 编号处置位**。准备完毕后发起调用，传入如上所述指定参数，程序开始阻塞等待 select 返回。当有至少一个数据准备就绪时，在内核中**将对应操作未就绪的 fd 编号处置空**，函数返回。用户程序可以根据返回值判断是否发生错误，若无错误，遍历传入的 fd 编号，若准备就绪则执行相应操作。
### 2.1.2 使用示例
```c
// 初始化套接字
sockfd = socket(AF_INET, SOCK_STREAM, 0);
memset(&addr, 0, sizeof (addr));
addr.sin_family = AF_INET;
addr.sin_port = htons(2000);
addr.sin_addr.s_addr = INADDR_ANY;
bind(sockfd,(struct sockaddr*)&addr ,sizeof(addr));
listen (sockfd, 5); 

for (i=0;i<5;i++) {
  memset(&client, 0, sizeof (client));
  addrlen = sizeof(client);
  // 设置fd编号
  fds[i] = accept(sockfd,(struct sockaddr*)&client, &addrlen);
  if(fds[i] > max)
  	max = fds[i];
}

while(1){
  // 将rset置空，因为每一轮操作都会改变rset，需要重新置空并赋值
  FD_ZERO(&rset);
  for (i = 0; i< 5; i++ ) {
    // 将对应fd编号处置位
  	FD_SET(fds[i],&rset);
  }
  
  puts("round again");
  // 程序阻塞在此处，直到至少有一个io准备就绪，或发生错误
  select(max+1, &rset, NULL, NULL, NULL);
  
  // 时间复杂度为o(n)
  for(i=0;i<5;i++) {
    // 判断是否准备就绪，为1则就绪
  	if (FD_ISSET(fds[i], &rset)){
  		memset(buffer,0,MAXBUF);
  		read(fds[i], buffer, MAXBUF);
  		puts(buffer);
  	}
  }	
}
```
### 2.1.3 存在问题

1. set 大小固定
2. set 无法重用，每次调用 select 前必须重新置空置位一遍。
3. select 参数较多，用户向内存的拷贝负载较大。
4. 返回后用户无法感知哪些文件准备就绪，需要遍历一遍，时间复杂度 **o(n)**。
5. 使用时每次调用都会发生用户到内核的拷贝。
## 2.2 poll
poll 使用结构体 pollfd 替代了 select 中的位图。
### 2.2.1 数据结构/函数原型
```c
#define POLLIN          0x0001
#define POLLPRI         0x0002
#define POLLOUT         0x0004
#define POLLERR         0x0008
#define POLLHUP         0x0010
#define POLLNVAL        0x0020
```
```c
struct pollfd {
    // fd编号
	int fd;
    // 指定事件
	short int events;
    // 返回是否就绪
	short int revents;
};
```
```c
// fds 文件事件结构体
// nfds fds长度
// timeout 超时时间
int poll(struct pollfd *fds, int nfds, int timeout)
```
用户程序创建一组 pollfd 结构体，将 pollfd 中 fd 设为 fd 编号，**在 events 处订阅所需操作**，可通过或操作同时订阅多个事件。随后发起调用，传入如上所述指定参数，程序开始阻塞等待 poll 返回。当有至少一个数据准备就绪时，在内核中**将 pollfd 中的 revents 对应的操作为置位**，函数返回。用户程序可以根据返回值判断是否发生错误，若无错误，循环 fd 总数，根据 revents 判断是否就绪，若就绪执行相应操作。
### 2.2.2 使用示例
```c
for (i=0;i<5;i++){
  memset(&client, 0, sizeof (client));
  addrlen = sizeof(client);
  // 设置fd编号
  pollfds[i].fd = accept(sockfd,(struct sockaddr*)&client, &addrlen);
  // 设置POLLIN事件，即读事件
  pollfds[i].events = POLLIN;
}

sleep(1);
while(1){
  puts("round again");
  // 程序阻塞在此处，直到至少有一个io准备就绪，或发生错误
  poll(pollfds, 5, 50000);

  // 遍历fd总数
  for(i=0;i<5;i++) {
    // 通过revents和POLLIN与操作，可得到是否就绪
    if (pollfds[i].revents & POLLIN){
        // 判断就绪以后将revents置空，不影响下次poll执行
		pollfds[i].revents = 0;
		memset(buffer,0,MAXBUF);
		read(pollfds[i].fd, buffer, MAXBUF);
		puts(buffer);
	}
  }
}
```
### 2.2.3 存在问题
相较于 select，poll 突破了 set 大小固定的限制，并在 poll 中重用结构体的代价更小，并且调用 poll 传入的参数更少了，但依旧存在如下问题：

1. 返回后用户无法感知哪些文件准备就绪，需要遍历一遍，时间复杂度 **o(n)**。
2. 使用时每次调用都会发生用户到内核的拷贝。
## 2.3 epoll
epoll 基本解决的 select 和 poll 中存在的问题，是 linux 多路复用模型的首选。
### 2.3.1 数据结构/函数原型
SYSCALL_DEFINEx 为源码中的调用方式，相同实现在其下一行。
```c
#define EPOLLIN		    (__force __poll_t)0x00000001
#define EPOLLPRI	    (__force __poll_t)0x00000002
#define EPOLLOUT	    (__force __poll_t)0x00000004
#define EPOLLERR	    (__force __poll_t)0x00000008
#define EPOLLHUP	    (__force __poll_t)0x00000010
#define EPOLLNVAL	    (__force __poll_t)0x00000020
#define EPOLLRDNORM	    (__force __poll_t)0x00000040
#define EPOLLRDBAND	    (__force __poll_t)0x00000080
#define EPOLLWRNORM	    (__force __poll_t)0x00000100
#define EPOLLWRBAND	    (__force __poll_t)0x00000200
#define EPOLLMSG	    (__force __poll_t)0x00000400
#define EPOLLRDHUP	    (__force __poll_t)0x00002000
#define EPOLLEXCLUSIVE	((__force __poll_t)(1U << 28))
#define EPOLLWAKEUP	    ((__force __poll_t)(1U << 29))
#define EPOLLONESHOT	((__force __poll_t)(1U << 30))
// 边缘触发
#define EPOLLET		    ((__force __poll_t)(1U << 31))
```
```c
struct epoll_event {
	__poll_t events;
	__u64 data;
} EPOLL_PACKED;
```
```c
SYSCALL_DEFINE1(epoll_create, int, size)

// size 已废弃
// return epoll的fd编号
int epoll_create(int size)
```
```c
#define EPOLL_CTL_ADD 1
#define EPOLL_CTL_DEL 2
#define EPOLL_CTL_MOD 3

SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd, struct epoll_event __user *, event)

// epfd 通过epoll_create创建的epoll fd编号
// op 执行的操作
// fd 操作目标的fd编号
// event 操作事件
int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event)
```
```c
SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events, int, maxevents, int, timeout)

// epfd 通过epoll_create创建的epoll fd编号
// events 接收返回数组
// maxevents 处理事件的最大数
// timeout 超时时间
int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout)
```
### 2.3.2 使用示例
```c
// 新建一个events数组，用于接收
struct epoll_event events[5];
// 创建epoll，返回fd编号
int epfd = epoll_create(10);

for (i=0;i<5;i++) {
  static struct epoll_event ev;
  memset(&client, 0, sizeof (client));
  addrlen = sizeof(client);
  // 标记data为fd编号
  ev.data = accept(sockfd,(struct sockaddr*)&client, &addrlen);
  // 设置需要的事件
  ev.events = EPOLLIN;
  // 通过EPOLL_CTL_ADD将当前event添加进epoll
  epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data, &ev); 
}

while(1){
  puts("round again");
  // 等待返回结果，nfds为就绪文件的数目
  nfds = epoll_wait(epfd, events, 5, 10000);
  
  // 遍历nfds次，时间复杂度仅为n(1)
  for(i=0;i<nfds;i++) {
  		memset(buffer,0,MAXBUF);
        // 从events[i]读取就绪文件，执行操作
  		read(events[i].data.fd, buffer, MAXBUF);
  		puts(buffer);
  }
}
```
## 2.4 epoll 源码解析
### 2.4.1 内部数据结构
```c
struct eventpoll {
	/*
	 * This mutex is used to ensure that files are not removed
	 * while epoll is using them. This is held during the event
	 * collection loop, the file cleanup path, the epoll file exit
	 * code and the ctl operations.
	 */
	struct mutex mtx;

	/* Wait queue used by sys_epoll_wait() */
	wait_queue_head_t wq;

	/* List of ready file descriptors */
	struct list_head rdllist;

	/* Lock which protects rdllist and ovflist */
	rwlock_t lock;

	/* RB tree root used to store monitored fd structs */
	struct rb_root_cached rbr;

	/* wakeup_source used when ep_scan_ready_list is running */
	struct wakeup_source *ws;

	/* The user that created the eventpoll descriptor */
	struct user_struct *user;

	struct file *file;

};
```
epoll 底层的核心结构结构体，以文件的形式存在。

---

```c
struct epitem {
	union {
		/* RB tree node links this structure to the eventpoll RB tree */
		struct rb_node rbn;
		/* Used to free the struct epitem */
		struct rcu_head rcu;
	};

	/* List header used to link this structure to the eventpoll ready list */
	struct list_head rdllink;

	/*
	 * Works together "struct eventpoll"->ovflist in keeping the
	 * single linked chain of items.
	 */
	struct epitem *next;

	/* The file descriptor information this item refers to */
	struct epoll_filefd ffd;

	/* List containing poll wait queues */
	struct eppoll_entry *pwqlist;

	/* The "container" of this item */
	struct eventpoll *ep;

	/* List header used to link this item to the "struct file" items list */
	struct hlist_node fllink;

	/* wakeup_source used when EPOLLWAKEUP is set */
	struct wakeup_source __rcu *ws;

	/* The structure that describe the interested events and the source fd */
	struct epoll_event event;
};
```
每一次添加操作将抽象成一个 epitem，存入 eventpoll

---

```c
struct ep_pqueue {
	poll_table pt;
    // 绑定的epi
	struct epitem *epi;
};

typedef struct poll_table_struct {
    // epoll_wait触发函数
	poll_queue_proc _qproc;
    // 事件
	__poll_t _key;
} poll_table;
```
### 2.4.2 epoll_create 核心实现
```c
static int do_epoll_create(int flags) {
	int fd;
	struct eventpoll *ep = NULL;
	struct file *file;

    // 分配ep内存
	error = ep_alloc(&ep);

    // 新建fd和file，绑定到内部ep上
	fd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));
    // anon_inode_getfile中将ep绑定到file->private_data上
	file = anon_inode_getfile("[eventpoll]", &eventpoll_fops, ep, O_RDWR | (flags & O_CLOEXEC));

	ep->file = file;
    // 将fd和file关联
	fd_install(fd, file);
	return fd;
}
```
do_epoll_create 完成了内部 eventpoll 结构体的内存分配，创建新的 fd 和 file，eventpoll 在内部的存在形式就是一个**文件**，最后返回 eventpoll 文件的 fd 编号。
### 2.4.3 epoll_ctl 核心实现
```c
int do_epoll_ctl(int epfd, int op, int fd, struct epoll_event *epds, bool nonblock) {
	int error;
	int full_check = 0;
	struct fd f, tf;
	struct eventpoll *ep;
	struct epitem *epi;
	struct eventpoll *tep = NULL;

	// 根据fd编号拿到对应的ep文件和target文件
	f = fdget(epfd);
	tf = fdget(fd);

	// 经过一系列检查和判断（代码已省略），可以安全地拿到内部ep
	ep = f.file->private_data;

	epoll_mutex_lock(&ep->mtx, 0, nonblock);

	// 在红黑树中寻找对应的epi
	epi = ep_find(ep, tf.file, fd);

	switch (op) {
	case EPOLL_CTL_ADD:
		if (!epi) {
			epds->events |= EPOLLERR | EPOLLHUP;
			// 将新的epi加入到eventpoll
			ep_insert(ep, epds, tf.file, fd, full_check);
		} 
		break;
	case EPOLL_CTL_DEL:
		if (epi) {
			// 删除指定的epi
			ep_remove(ep, epi);
		}
		
		break;
	case EPOLL_CTL_MOD:
		if (epi) {
			// 非EPOLLEXCLUSIVE模式
			if (!(epi->event.events & EPOLLEXCLUSIVE)) {
				epds->events |= EPOLLERR | EPOLLHUP;
				// 修改指定的epi
				ep_modify(ep, epi, epds);
			}
		} 
		break;
	}
	mutex_unlock(&ep->mtx);

	fdput(tf);
	fdput(f);
	return error;
}
```
do_epoll_ctl 中首先获取了内部 ep 文件和目标文件，从内部 ep 文件拿到 ep 结构体，通过 ep_find 在 ep 红黑树中找到对应目标文件的 epi，再根据给定的操作，分别执行对应操作，最后 put 内部 ep 文件和目标文件，返回错误。

---

```c
static int ep_insert(struct eventpoll *ep, const struct epoll_event *event, struct file *tfile, int fd, int full_check) {
	__poll_t revents;
	struct epitem *epi;
	struct ep_pqueue epq;

	// 初始化epi
	INIT_LIST_HEAD(&epi->rdllink);
	epi->ep = ep;
	ep_set_ffd(&epi->ffd, tfile, fd);
	epi->event = *event;
	epi->next = EP_UNACTIVE_PTR;
	
	// 插入红黑树实现
	ep_rbtree_insert(ep, epi);

	// 将epi和epq绑定
	epq.epi = epi;

	// init_poll_funcptr将ep_ptable_queue_proc注册为epoll_wait回调函数
    // ep_ptable_queue_proc把epi对应fd事件到来时的回调函数设为ep_poll_callback
    // ep_poll_callback把就绪的epi放到就绪链表上
	init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);

	// 判断事件是否已经就绪，revents接收就绪事件
	revents = ep_item_poll(epi, &epq.pt, 1);

	write_lock_irq(&ep->lock);

	// 若就绪，将epi加入到就绪队列中
	if (revents && !ep_is_linked(epi)) {
		list_add_tail(&epi->rdllink, &ep->rdllist);
		ep_pm_stay_awake(epi);

		// 通知wait
		if (waitqueue_active(&ep->wq)) {
			wake_up(&ep->wq);
		}
	}

	write_unlock_irq(&ep->lock);

	return 0;
}
```
ep_insert 初始化一个 epiteam，并将它加入到红黑树中，然后将 epi 绑定到新建的 epq 上，**在 epq 中注册回调函数**，此时查看事件是否已经就绪，就绪的话加入到 ep 上的就绪队列。
### 2.4.2 epoll_wait 核心实现
```c
static int do_epoll_wait(int epfd, struct epoll_event __user *events, int maxevents, struct timespec64 *to) {
	struct fd f;
	struct eventpoll *ep;

	// 获取内部ep文件
	f = fdget(epfd);
	// 获取ep结构体
	ep = f.file->private_data;

	// 寻找事件发生
	ep_poll(ep, events, maxevents, to);

	fdput(f);
	return error;
}
```
```c
static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, struct timespec64 *timeout) {
	eavail = ep_events_available(ep);
	while (1) {
		if (eavail) {
            // epoll的主要实现在此处
			res = ep_send_events(ep, events, maxevents);
		}
	}
}
```
```c
static int ep_send_events(struct eventpoll *ep, struct epoll_event __user *events, int maxevents) {
	struct epitem *epi, *tmp;
	LIST_HEAD(txlist);
	poll_table pt;
	int res = 0;

	init_poll_funcptr(&pt, NULL);

	mutex_lock(&ep->mtx);
	// 将ep->rdllist连接到txlist，并清空epi->rdllink
	ep_start_scan(ep, &txlist);

	// 遍历txlist
	list_for_each_entry_safe(epi, tmp, &txlist, rdllink) {
		__poll_t revents;

		// 超出用户传入的最大event数，退出循环
		if (res >= maxevents)
			break;

		// 判断事件是否已经就绪，revents接收就绪事件
		revents = ep_item_poll(epi, &pt, 1);
		if (!revents)
			continue;

		// 将epi->event.data存入用户events数组开头的data，将revents存入用户events数组开头的events，并且将地址后移一位，即指向数组的下一位
		events = epoll_put_uevent(revents, epi->event.data, events);

		// 若用户给定的events长度不够，退出循环
		if (!events) {
			list_add(&epi->rdllink, &txlist);
			ep_pm_stay_awake(epi);
			break;
		}
		// 每成功就绪一个fd的一组或一个事件，res加一
		res++;
	}
	
	ep_done_scan(ep, &txlist);
	mutex_unlock(&ep->mtx);

	return res;
}
```
ep_send_events 获取就绪队列并放入 txlist，遍历列表，对其中的每一个 epi 执行对应 ep_item_poll，接收就绪事件，通过 epoll_put_uevent 将就绪事件和 data（此处可以明白 data 的作用是辨别一开始执行 epoll_ctl 的目标文件是哪个，一般可以直接存放 fd 编号）依次放入用户传入的 events 结构体头部，并返回成功放入的个数。
## 2.5 kqueue
# 3 netpoll
## 初始化
```go
func netpollinit() {
    // 新建一个epoll
    epfd = epollcreate1(_EPOLL_CLOEXEC)
    if epfd < 0 {
        epfd = epollcreate(1024)
        if epfd < 0 {
            println("runtime: epollcreate failed with", -epfd)
            throw("runtime: netpollinit failed")
        }
        closeonexec(epfd)
    }
    // 新建一个非阻塞管道，管道可用于netpollBreak中断多路复用
    r, w, errno := nonblockingPipe()
    if errno != 0 {
        println("runtime: pipe failed with", -errno)
        throw("runtime: pipe failed")
    }
    // 创建一个读事件
    ev := epollevent{
        events: _EPOLLIN,
    }
    *(**uintptr)(unsafe.Pointer(&ev.data)) = &netpollBreakRd
    // 监听管道读端的读事件
    errno = epollctl(epfd, _EPOLL_CTL_ADD, r, &ev)
    if errno != 0 {
        println("runtime: epollctl failed with", -errno)
        throw("runtime: epollctl failed")
    }
    netpollBreakRd = uintptr(r)
    netpollBreakWr = uintptr(w)
}
```
# 参考文献

- UNIX网络编程 卷1：套接字联网API（第3版）
- [https://strikefreedom.top/archives/go-netpoll-io-multiplexing-reactor](https://strikefreedom.top/archives/go-netpoll-io-multiplexing-reactor)
- [https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll](https://devarea.com/linux-io-multiplexing-select-vs-poll-vs-epoll)
- [https://cloud.tencent.com/developer/article/1373483](https://cloud.tencent.com/developer/article/1373483)
- [https://icoty.github.io/2019/06/03/epoll-source/](https://icoty.github.io/2019/06/03/epoll-source/)
# TODO

- [ ] epoll源码部分理解不够深刻，待重写
- [ ] 2.5 kqueue
